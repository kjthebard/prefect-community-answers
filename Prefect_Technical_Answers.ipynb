{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbff595e-f1a3-4e7e-ae3c-c8ef34845241",
   "metadata": {},
   "source": [
    "\n",
    "# 1. What is the difference between a class and a function? When is it appropriate to use one or the other? \n",
    "\n",
    "## Functions\n",
    "\n",
    "A function performs a single or series of instructions given expected inputs. When you give a function the expected inputs, the function will run and perform the computation as designed, nothing more, nothing less. They are excellent if you want to perform a simple task. Below is an example of a function that sums two numbers. I have also provided an example of using a function to strip the 'd' character  instead of remembering the regex for the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e11cb6-00af-4a62-9f2e-4b985cb842b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the sum is: 20\n",
      "The original sentence: dang that's rad! was changed to: tang that's rat!\n"
     ]
    }
   ],
   "source": [
    "## Function Example #1\n",
    "# A function that sums two numbers together and returns the \n",
    "\n",
    "def two_sum(a:int, b:int):\n",
    "    return a + b\n",
    "\n",
    "summed = two_sum(9, 11)\n",
    "print(\"The result of the sum is: \" + str(summed))\n",
    "\n",
    "\n",
    "## Function Example #2\n",
    "# The function .replace is a rad function that can remove any letter or series of letters from a string!\n",
    "\n",
    "sentence = \"dang that's rad!\"\n",
    "mutated = sentence.replace('d', 't')\n",
    "print(\"The original sentence: \" + sentence +  \" was changed to: \" + mutated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3b499-4647-45a9-a1d4-50d1648dd7eb",
   "metadata": {},
   "source": [
    "## Example Explanation\n",
    "\n",
    "As you can see from these examples, a function performs a task given you have the right inputs. You would not expect either function to run if missing inputs or given malformed inputs. The first function would fail if you gave it anything other than an integer for either number because that is the way the user defined it. The function only accepts integers and it only accepts two numbers, however; any function a user defines can be given more flexibility to accept a wide variety of data types and define as many or as few inputs as the function needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0385a-9153-4177-a0d2-e67edbf6ef8f",
   "metadata": {},
   "source": [
    "## Classes \n",
    "\n",
    "Classes are a different animal all together. A Class at it's core is trying to define 'something'. That something could be a Car, an Animal, or even a Droid. I think this is why I struggled with them mightily when I first learned about them. They express an abstraction of thought. It allows a user to inherit properties from an 'abstraction' or base class and define child classes with new features and mutations unique to those child classes. This allows for shared properties to belong to the same archetype. \n",
    "\n",
    "### Cats, Dogs and Bears, Oh my!\n",
    "Let's use animals as a real life example to describe classes. Cats, Dogs, and Bears all have four legs, are defined as mammals, and have paws. However, Cats are the only species among those listed with retractable claws, this is a property unique to them. All of these animals share common traits but are not the same and have very distinct features/properties that allow us to classify them as a dog, or a cat or a bear. This is the core power behind a class. We can define a base that allows others to inherit their properties and, allow for greater definition without having to rewrite code over and over again.\n",
    "\n",
    "### Star Wars Droids Example\n",
    "This is getting a bit wordy so let me provide an example that will give an idea of how Classes work. In this example, I will create a 'Droid' class where I will show how you can create a class that 'inherits' the behaviors of the Base Droid class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1825c375-14cb-4b58-bc61-d791c9dd632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh no, my movement is undefined\n",
      "This droid has not been given a communication style, please initialize the style\n"
     ]
    }
   ],
   "source": [
    "class Droid():\n",
    "    \n",
    "    def __init__(self, cpu, gpu, power, comms, legs):\n",
    "        \n",
    "        # These are properties unique to the Droid class, I have defined them in the initialized part\n",
    "    \n",
    "        self.cpu = cpu\n",
    "        self.gpu = gpu\n",
    "        self.power = power\n",
    "        self.communication = comms\n",
    "        self.legs = legs\n",
    "    \n",
    "    def communicate(self):\n",
    "        # Definition of the droid subroutine, to be overwritten in higher definitions of droids\n",
    "        # takes the comm style of the droid, however this is not assigned.\n",
    "        if self.communication == None:\n",
    "            print(\"This droid has not been given a communication style, please initialize the style\")\n",
    "        else:\n",
    "            print(self.communication)\n",
    "            \n",
    "    def move(self):\n",
    "        # Defined movement styles for droids, for this example I am only defining three styles of movement\n",
    "        # There could be more or less, this is just to show how a droid can inherit a function.\n",
    "        \n",
    "        if self.legs == 1:\n",
    "            print(\"hop, hop, hop\")\n",
    "        elif self.legs == 2: \n",
    "            print(\"left, right, left\")\n",
    "        elif self.legs == 3:\n",
    "            print(\"tripod with wheels\")\n",
    "        else:\n",
    "            print(\"oh no, my movement is undefined\")\n",
    "            \n",
    "# Call the Droid\n",
    "D = Droid(cpu=None,gpu=None,power=None,comms=None,legs=None)\n",
    "D.move()\n",
    "D.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126ad94-545a-4681-a318-35062d75b809",
   "metadata": {},
   "source": [
    "### Base Droid Class\n",
    "\n",
    "This class above has 5 innate attributes: cpu, gpu, power, communication style and how many legs. These are attributes every droid has. Every future droid we define must have these 5 things. This is what is known as inheritance. Think of this sort of like the DNA of an animal, in so far as it inherits all of the properties or 'genes' of the parent.\n",
    "\n",
    "I have also defined methods that have explicit logic based on future droid definitions. This allows us to program base droid features without having to code it every single time for every single droid in the future. \n",
    "\n",
    "Here is an example of what that looks like: let's build everyone's favorite droid R2D2! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "285edd18-4658-4aaf-8683-f886d1940717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beep, Boop, Bweep!\n",
      "tripod with wheels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sassy'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class R2D2(Droid):\n",
    "    \"\"\"\n",
    "    A class for defining an R2D2 unit composed of cpu, gpu, power, communicatiom, style, and legs from \n",
    "    the Droid Base Class and an attitude that can be set when initializing the R2D2 Droid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cpu, gpu, power, comms, legs, attitude):\n",
    "        self.attitude = attitude\n",
    "        super().__init__(cpu, gpu, power, comms, legs)\n",
    "        \n",
    "    def set_comms(self, comms):\n",
    "        self.communication = comms\n",
    "    \n",
    "    def set_movement(self, leg_num):\n",
    "        self.legs = leg_num\n",
    "    \n",
    "    def attitude(self):\n",
    "        print(self.attitude)\n",
    "\n",
    "## The r2d2 droid is initialized here and used below, notice we don't set comms here but later\n",
    "R = R2D2(cpu=\"m2\",gpu=\"r2gpu\",power=\"electric\",comms=None,legs=None, attitude=\"sassy\")    \n",
    "\n",
    "## Sets the comms and the legs\n",
    "R.set_comms(\"Beep, Boop, Bweep!\")\n",
    "R.set_movement(3)\n",
    "\n",
    "# Now we can run the methods in the Droid Base class as we have inherited these. Notice how these are\n",
    "# not defined anywhere in our R2D2 class!\n",
    "R.communicate()\n",
    "R.move()\n",
    "\n",
    "# We even can customize the class and give it attributes the Parent doesn't have!\n",
    "R.attitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d243ca-2e42-4a12-b266-ad152ceed1a5",
   "metadata": {},
   "source": [
    "### R2D2 Class \n",
    "\n",
    "So a couple of things here, First, we establish that this particular R2D2 can be programmed with a customizable personality. This was not explicitly stated anywhere in the original Droid class but, it is very easy to add new properties to the Children of the Parent Class. Second, the methods for *communicate()* and *move()* were not defined anywhere but in the Parent. This is a powerful paradigm as it allows for reuse of code when building similar abstractions. We could use this base Droid class to go through and build out a definition for every single droid in the Star Wars universe.\n",
    "\n",
    "### Why this matters?\n",
    "\n",
    "In summary, classes allow for powerful abstractions that fuel code reuseability and reduce developer need to hard code everything for every object they create. \n",
    "\n",
    "### Alright, so when should I use a functions vs. classes?\n",
    "\n",
    "Use functions when you want a single use piece of code that you can reuse agnostically. Use classes when you know you need to define an abstraction that can build new dependent objects/structures faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae35b7-5982-475d-b3fb-a3be21db4933",
   "metadata": {},
   "source": [
    "# 2. Explain the difference between the following code snippets - is one preferred over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e15ea918-0bc6-4368-a2e4-124707c31f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation 1\n",
    "f = open('my_file.txt', 'w')\n",
    "f.write('hi')\n",
    "f.close()\n",
    "\n",
    "#implementation 2 \n",
    "with open('my_file.txt', 'w') as f:\n",
    "    f.write('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733ad9b-4b0a-4cf0-ac22-35f56ca00e42",
   "metadata": {},
   "source": [
    "## The *with* statement and why it matters!\n",
    "\n",
    "Aside from the fact that the second implementation is more pythonic, the *with* statement matters for several reasons. The first reason is that the *with* statement handles file closing. There is nothing more heartbreaking than trying to access a file that is already locked by another process. Using *with* ensures that what ever you are doing to the file is closed upon leaving the indented code block.  It is also easier to spot in a large code file when reviewing as the indentation signals to the reader that a transaction block is occuring. I would pick the second implementation over the first every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e1a44-d724-486f-a095-541b9e5ff6d8",
   "metadata": {},
   "source": [
    "# 3. Describe three different methods for achieving parallelism in Python. What caveats or considerations should a developer be aware of with each?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaffc1-c92c-4e34-ac3f-a2cef20cec0b",
   "metadata": {},
   "source": [
    "Here are three ways I would parallelise my python code: \n",
    "\n",
    "## A. Multiprocessing and Multithreading\n",
    "\n",
    "Some people might consider these two separate things. However, they are often discussed together when it comes to speeding upising as their API are very similarly defined in Python. I used these at the beginning of my career and if I'm honest I haven't used much as I've typically used either a Spark variant(Apache Spark, Databricks, AWS Glue) or Dask (either locally or by using the k8s-dask cluster deployed in AWS). This is a great excuse to brush up on pools, maps, threads, so let's jump in the pool!\n",
    "\n",
    "### i. Multiprocessing\n",
    "\n",
    "Multiprocessing at its core is pretty much stated in the name, multiple processes. Multiprocessing  utilizes the processors of the CPU to achieve parallelism through concurrency. This often involves defining a function and mapping said function to a pool of workers. The CPU then assigns 1 worker core to 1 process to execute each process. \n",
    "\n",
    "#### Caveats for multiprocessing\n",
    "\n",
    "As you are controlling cpu cores rather than threads, one important thing to keep in mind is that it is much harder to share data resources in a Multiprocessing scenarios, due to blocking and contention which will eventually lead to deadlocks. It is often considered best practice if you must share data it is better to  share messages by utilizing the multiprocess.Queue objects. A developer must also clean up after themselves and manage messaging and logging to ensure that their program can run without locking up your subprocesses and will lead to increased development time. \n",
    "\n",
    "### ii. Multithreading\n",
    "\n",
    "Multithreading tries to take advantage of the the threads available in your computer via context switiching. When a thread is interrupted, the context is saved and spun up in a new thread. This gives the apperaence of parallelism but are actually not truely parallel. They have an advantage as multiple threads can share the same process and thusly are allowed to share the same data area of your machine. This allows them to communicate with each other more effectively than if they were spun into multiple processes. They are also cheaper than processing from a memory perspective.\n",
    "\n",
    "#### Caveats for multithreading\n",
    "Something developer caveats are that a developer needs to consider how their code will impact the Global Interactive Lock (GIL). You can define your program in such a way that could lock you out of your threads. When Multithreading, a developer must also consider the queue, something the other two methods I am going to dive into either abstract or handle in different ways for you.  Again, you must handle logging, handleing of exceptions at a much tighter level than you want and thus increases development time when developing maintainable and scalable code.\n",
    "\n",
    "## B. PySpark\n",
    "\n",
    "Pyspark is an API layer that interfaces with Apache Spark. There are many advantages for using Spark over  other data processing tools. Neither of the two previous examples scale well when handling truly big data (Terabytes and above). This is because they were not designed with those sorts of datasets in mind. To even get close to the level of parallelism that Spark can achieve you would have to split the data into much smaller files and create a system to manage which files have and have not been read.  Spark handles this work on its .read method in its entirity, abstrating the need to deal with a cluster as many machines and allowing a user to interface withb clusters as if talking to one machine. This greatly reduces the need to manage resources, decreasing developer costs by reducing the time needed to write performant and scalable code. Spark also allows for horizontal scaling, in that if your code sucks, you can add more computers. This comes with the caveat that as you add more computers you will increase your costs at a much faster rate, but will introduce complexity  \n",
    "\n",
    "Spark also extends the MapReduce paradigm and allows for more general DAGS (directed acyclic graphs) to be performed. This is a huge advantage over traditional MapReduce as previously you would have had to write the results out to a distributed file system. Spark allows a user to lazily approach coding and sequence or chain several transformations together that makes it faster than even older parallel methods of munging data. \n",
    "\n",
    "Spark also has nice predefined functions for common transformational patterns used in Big Data processing. This layer of optimization allows for developers to code at a much quicker pace when they take advantage of the predefined functions. It also is one of the more mature Data Processing tools, meaning there is lots of help available online if you get stuck. Basic ML and Traditional SQL logic thrive on Spark and there is even a .sql interpreter that both the Pyspark and Spark Scala APIs can interface and understand sql. You can even save temp tables in the cluster and switch back and forth between Pyspark and Spark Scala as needed which can be very powerful. If you have to define a custom udf this is the route I would approach. Save the data with Pyspark using a temp table and then write a custom udf in scala to munge data with better performance.\n",
    "\n",
    "Spark also has a mature streaming package that interfaces nicely with Kafka without much need for custom programming. The community also has several wonderful integrations with cloud tools and many implementations that you can run Spark on. Databricks offers a SaaS approach to Spark development, geared at faster development, business insights and some lightweight machine learning. \n",
    "\n",
    "### Caveats for Spark\n",
    "\n",
    "To achieve any of these wonderful speed gains, you need to run on a cluster which can become expensive if your code is not well optimized. If you have to define a custom function, a user defined function will always be slower, especially when using the Pyspark API than a native funtion that is already available in Spark. \n",
    "\n",
    "Pyspark is an API, ontop of an API, on top of an API. That means if you write Pyspark code, the compiler has to translate the code 3 times before it can execute. This extra layer of abstraction will slow down your program dramatically. In the past, I have avoided udfs like the plague as they can take a fast data processing pipeline and if not designed carefully, can force the computation onto the Orchestration/Driver node and away from the Compute/Worker nodes. This will cause your program to perform sluggishly. Spark handles a lot of things, but a developer needs to be more concious of the cluster, the resources and how their code will work in a Distributed system. \n",
    "\n",
    "This is especially critical when developing ml workflows as Spark does not interface well with current standard python ml packages. (At least not natively) Unless you are willing to wait for support, you may have to define those models and codes yourself. This would lead to further slowdown.\n",
    "\n",
    "When building ml pipelines, I tend to shy away from Spark for 2 reasons. The first is being rather dependent on the Spark community to add packages to the spark-ml package. I will admit there have been many improvements in recent years, but waiting for these common ml python packages to be introduced to the ecosystem can be tedious at times. The second is that there is already a tool that can interface with modern python ml packages and has been gaining traction for a while now. This is the last method for parallel processing in python I wish to discuss. The tool? Why it's Dask!\n",
    "\n",
    "## Dask\n",
    "\n",
    "Dask is the newest member to big data parallelism and, in my opinion, will upend Spark's decade of dominance over big data processing. Dask is: Easy to use, mimics the pandas dataframe model, allows for tight integrations with other python ml packages, and is just as easy to run locally as it is to send to a cluster to perform compute. This new tool has many great new features that help dataframes process at speeds close to Spark without the limitation of having to use predefined functions. If I were to boil down why dask is so great it would be the BYOF or Bring your own function feature. \n",
    "\n",
    "Dask has many uses outside of just this example. For those who look at the Pyspark API as 'too foreign' or are unwilling to learn it as they have gotten used to pandas, good news: dask mimics pandas functionality throughout its stack and makes it very easy for those who love pandas. There are also other ways to interface with data by using Dask Bags and Dask Arrays to help speed up computation if a developer so chooses. Debugging and Serialization are also easier as Dask is a native python library itself. Dask also allows for greater control over DAGs which might be enough of a reason to leave Spark in the dust if you really dislike Java/JVM managed MapReduce model. Dask also allows you to lazily evaluate python code in a similar way to Spark code. \n",
    "\n",
    "### Caveats for Dask\n",
    "\n",
    "If you need to do heavy ml workloads and scale your current python models. I would pick dask over spark almost every single time. There are some exceptions, specifically when it comes to streaming. dask relies on the futures module when it comes to streaming. While it is true that you can express greater control over how your stream functions; this would frustrate most users coming from Spark where the low level control is handled for you outside of a few spark configs you can set in your readStream method. \n",
    "\n",
    "The community is among the fastest growing in the python community, but has yet to see widespread adoption. Help on the internet can be hard to find for niche usecases, which can lead to longer development times. However, I do not see this being the case for long, as companies start to really approach/tackle harder ml problems; they will find the spark-ml package to be underwhelming or unable to meet their growing desire to effectively use ml at scale. This is especially important, seeing how dask will require more bespoke code to handle things Spark can do natively, such as the extension of the map-shuffle-reduce pardigm. I foresee a bunch of pacakges being developed around generalizing they harder more custom pieces of code as those problems become more commonplace in the dask space. \n",
    "\n",
    "## Summary \n",
    "\n",
    "- I would use Multiprocessing and Multithreading if my data is small and I require a significant speed boost for functions pandas handles poorly. \n",
    "- I would use Spark when my data is big, but I don't see my use cases growing outside of the MapReduce paradigm, or I require a stream procesing engine that doesn't require much bespoke code to run.\n",
    "- I would use Dask when trying to use ml at scale, or if I am doing ml on the edge, where I may need a bespoke stream processing solution. I would even use dask over multiprocessing and multithreading locally  as it is close to pandas and I don't have to think about locks for all but extreme edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defadb90-cce4-493c-bcf0-837c8af5dde8",
   "metadata": {},
   "source": [
    "# 4: Give an example of use cases when you would use a processes vs. when you would use threads in Python?\n",
    "\n",
    "I would use a process when I do not need to share data amoung my functions. As stated in my earlier answer around parallelism, it is extremly difficult to share data between processes/subprocess and is often easier to share messages if you need to share metadata. On a personal note, I find processes tend to be easier to read from a code review perspective and require less management overall. If you do not need to share data and are willing to give up some speed, I would use a process in this use case.\n",
    "\n",
    "Threads on the other hand, require a more seasoned hand to truly get the most out of them. You need to be careful that you do not lock your system or share too much or too little data among threads as that can cause a massive performance decrease. They also provide a way to share data far more easily than would be needed in a process. If my use case required that I share data around, I would use threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a43f1-13af-4afa-8ad0-46b2eb456a66",
   "metadata": {},
   "source": [
    "# 5: Pick a data tool that you are familiar with and that you think would integrate well with Prefect. Describe a use case you think it would be a good fit for, and another usecase that it would not be a good fit for. \n",
    "\n",
    "Data build tool (dbt) and Prefect are a powerful combination when trying to deliever reliable, sustainable, and fast Analytics. Scheduling traditional data pipelines using flows would allow for tight control over your dimensional models and Data Vault 2.0 Patterns and fuel insights on an OLAP Database. It would also allow for what is traditionally a forked approach with Data Science and Data Engineering efforts to combine into a Unified Data Analytics stack. You can use dbt to handle the generalized Data Engineering Efforts and to set up your feature store, while using dask to run your ml models on a cluster and write back to the database of your choice. All of this could be monitored from your prefect pipeline, where you could orchestrate, monitor, debug, and analyze your pipeline from a the UI. For ELT/ETL workflows that don't require an ingestion of a stream in real time. Prefect and dbt just works well in these scenarios. \n",
    "\n",
    "This model breaks down when talking about streaming/near-realtime. I may be unfamilar, but I don't see how Prefect handles monitoring of streams. Lazerus doesnt measure heartbeats faster than a minute so I know the agent would break down here on that fact alone. dbt is also a poor choice for streams as it is traditionally used for standard database SQL management and data cataloging. I would pick a monitoring tool such as Prometheus and would use a realtime db like influxdb or a key value store like redis for this work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de37c1d-30aa-4f73-bccb-1b0459239597",
   "metadata": {},
   "source": [
    "# 6: Suppose a non expert user is requesting advice and guidence on setting up their first distributed environment - what recommendations would you provide?\n",
    "\n",
    "My first advice would be to run everything locally, run dask locally via kubernetes and ensure that your code works. 90% of the time people try to move too quickly to a distributed environment when there are bugs in their local code. This will save time and money as you do not want to debug things in a disitributed environment that you could have debugged locally.\n",
    "\n",
    "Assuming the first step is done, I think it would depend on what data ecospace that user lives in. It would vary based on if they were cloud or not. If they are cloud it would vary by cloud, If they are in one of the Big 3 (AWS, Azure, GCP) I would recommend deploying a dask-k8s cluster with autoscaling turned off initially to one of their managed kubernetes services, assuming they have confirmed their code runs locally in dask first. I would say to start small and complete some test runs with a small cluster as k8s services can be very expensive if you let the services autoscale out by themseleves. I would also recommend that as soon as they are done with compute they should turn off their clusters to avoid any wasted money from leaving a cluster on over the weekend. \n",
    "\n",
    "If running things on an on-prem cluster, I would ensure that the environment is able to run these things on the cluster by checking the documentation depending on which deployment they have. If they can't I would recommend a distributed computing strategy that makes sense given the cluster they are using. Then provide a list of tools as needed. I would sincerely hope those customers were migrating to the cloud as they could be missing out on cool new features that might be unsupported by an on-prem cluster, especially considering Prefect is designed with the cloud in mind!\n",
    "\n",
    "I would highly recommend a cloud deployment script to manage this deployment. There are lots of little steps that might not be captured if deployed manually. A terraform script will allow for other people to deploy their own distributed systems without introduciung human error in setting everything up by hand. There are some boilerplate examples that I would provide like these that would allow for a non expert get up and running without having to mess around too much with the respective cloud console such as:\n",
    "\n",
    "- https://registry.terraform.io/modules/dabble-of-devops-biodeploy/eks-jupyterhub/aws/latest\n",
    "- https://github.com/aws-ia/terraform-prefect-agent-ec2\n",
    "\n",
    "Working with a person who is familiar with terraform would also make this process easier for a non expert person to restart, redeploy and manage their solution without constant intervention from DevOps or taking crucial time away from a SRE. As the sage in the cave once said, *\"It's dangerous to go alone\"*, so you should always ask for some help from a technical person where it makes sense. \n",
    "\n",
    "However, if this question is about how to set up a distributed Prefect environment by themselves, I would highly recommend Prefect Cloud. A SaaS tool will save them the headache of tring to set someting like this themselves and is well worth the money the hours of engineering would take for a dev ops team to set up a prefect cli instance. You would have to deal with vpc's, databases, permissioning in the cloud console just to name a few. Plus, there are many features in Prefect Cloud that are unavailable using the open source deployment such as Team Management and Security Features that would not be available if you spun up a terraform deployment of the prefect cli + agent + dashboard yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1819c7-7ca9-4bb5-aab6-736cb2c7de5a",
   "metadata": {},
   "source": [
    "# 7: What if your code requires non-Python dependencies such as C or Java? How would you prevent dependecy conflicts with other code projects? \n",
    "\n",
    "There are several things one can do to help manage what is often referred to as dependecy hell. Isolating Python into its own environment using pip-env is a great solution if different codes need to be run seperately. You can establish a requirements.txt and only load the specific packages you need, right down to the specific version number. If you are worried about version drift, you can even specify a Pipfile.lock to lock down exactly what packages are allowed to be installed and what is not allowed. \n",
    "\n",
    "You can also create several virtualization containers to deploy your code to. I would highly recommend using containers in this way if you know there will be dependency conflicts between your Python, Java, and C++ code. By doing this, you are ensuring that Python code lives in one container, Java code in another and C++ in another again. This also has the added benefit of isolating your run environmnents as you will know for sure that only Python code ran at this step, then feeding the output to your next container to only run the C++ code you have and finally a container for running your Java code. You could even run all three at once or if they are dependent upon each others outputs schedule those runs with an orchestration tool such as Prefect! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9879886-a290-4f62-99cc-b87bd00b43d9",
   "metadata": {},
   "source": [
    "# 8. Name your three favorite Prefect features and describe the user problems or use cases they solve. \n",
    "\n",
    "## Use Case 1: Orchestration Code and Execution Code finally in Harmony with Flows\n",
    "\n",
    "This is my biggest complaint about airflow. It has always been a struggle to learn and teach others how dag files work. In airflow, you have to define a file that acts as a manifest for your job. This creates a separation of executable code from scheduling code. Back in 2017, I deployed my first Airflow cluster. I gotta say, it was a huge upgrade from chron. I could now schedule jobs just by using Airflow Operators in these dag files rather than run them by trying to predict when the chron jobs would end, or by trying to write some archaic bash magic that would allow me to schedule it. \n",
    "\n",
    "But Airflow's seperate Dag folder became a chore to manage once we started creating 100's of jobs. It even slowed down the cluster we worked on because we had so many jobs trying to open one folder to read the file it needed. \n",
    "\n",
    "Prefect allows you to write pythonic flows right inside your project. You can import code natively and scale out and submit code to clusters while defining your code in the project. No longer do you have to maintain a separate code repo just for your scheduler just for things to make sense. This is by far and away my favorite feature of Prefect. You even register the flow state with the agent so I/O issues of reading the same folder are no longer a concern. Such a great feature!\n",
    "\n",
    "## Use Case 2: UniversalRun: Fixing the pain of Deployments\n",
    "\n",
    "This is a close second favorite feature. Your code really doesn't have to change between deployments. How many times have we heard the common phrase: \"It work's on my machine!\" With UniversalRun you can run your code on and across multiple agents. This is so great for allievating the headache of code promotion and deployment as code that is written locally is almost guarenteed to work when deployed else where. This is what Prefect means when they say they engineer with \"negative engineering\" in mind. It is a mindset that is unique to top tech performers and I have to say it made me fall in love with Prefect from 2019 onwards.\n",
    "\n",
    "## Use Case 3:  Clocks, Cron evolved\n",
    "\n",
    "My final favorite feature is the amount of ways Prefect can understand scheduling. No longer do I need to remember exactly how to translate human dates into cron dates. This was always a struggle as I often find myself wasting time going to crontab.guru and looking how I need to schedule one particular job. Now I can define them as readable human dates. What a relief! I can even define a schedules time by a particular timezone. Gone is the need to translate all time into UTC, prefect handles the conversion for me. For developers who have struggled with timezones and timing, Prefects Clocks are a much needed relief and time save. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8d2bd-81ce-47c9-80b8-6411cb5f53e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
